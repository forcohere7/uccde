{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Underwater Image Enhancement: Colour Correction and Detail Enhancement using Hybrid Real-ESRGAN\n",
        "\n",
        "This notebook implements a pipeline for enhancing underwater images by combining **color correction techniques** with **Real-ESRGAN** for super-resolution. The process addresses common underwater imaging issues such as color distortion, low contrast, and loss of detail due to water attenuation. The pipeline consists of two main stages:\n",
        "\n",
        "1. **Color Correction**: Adjusts color balance and compensates for underwater-specific distortions using techniques like guided filtering and LAB color space stretching.\n",
        "2. **Detail Enhancement**: Applies Real-ESRGAN, a state-of-the-art super-resolution model, to enhance image details and sharpness.\n",
        "\n",
        "Additionally, the notebook includes a **performance evaluation** step to compute metrics (PSNR and MSE) by comparing processed images with reference images.\n",
        "\n",
        "## Objectives\n",
        "- Correct color casts in underwater images caused by light absorption and scattering.\n",
        "- Enhance image contrast and detail using advanced image processing and deep learning techniques.\n",
        "- Evaluate the quality of enhanced images using quantitative metrics.\n",
        "\n",
        "## Prerequisites\n",
        "- Google Colab with GPU support (T4 or better recommended).\n",
        "- Input images or videos in the `input/` directory.\n",
        "- Reference images in the `reference/` directory for evaluation.\n",
        "- Pre-trained Real-ESRGAN model weights (`net_g_5000.pth`) uploaded to Colab.\n",
        "\n",
        "Let's proceed with the implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies\n",
        "\n",
        "This section installs the required Python libraries for image processing, video handling, and Real-ESRGAN-based super-resolution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install ffmpeg-python\n",
        "!pip install -q basicsr facexlib gfpgan numpy opencv-python Pillow torch torchvision tqdm realesrgan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Fix Dependency Issues\n",
        "\n",
        "The `basicsr` library has a known import issue with `torchvision`. This script corrects the import statement in the `degradations.py` file to ensure compatibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile dependency-fix.sh\n",
        "#!/bin/bash\n",
        "# Fix torchvision import in basicsr/data/degradations.py\n",
        "sed -i 's/from torchvision.transforms.functional_tensor import rgb_to_grayscale/from torchvision.transforms.functional import rgb_to_grayscale/' /usr/local/lib/python3.11/dist-packages/basicsr/data/degradations.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!chmod +x dependency-fix.sh\n",
        "!./dependency-fix.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Color Correction Implementation\n",
        "\n",
        "This section defines the core classes and functions for underwater image color correction. The `GuidedFilter` class implements a guided filter for refining transmission maps, while the `ColourCorrection` class handles color balancing and depth map estimation. Additional functions enhance contrast and color through histogram stretching and LAB color space adjustments.\n",
        "\n",
        "Key features:\n",
        "- **Guided Filter**: Smooths transmission maps while preserving edges.\n",
        "- **Color Compensation**: Adjusts red and blue channels to counter underwater color casts.\n",
        "- **Depth Map Estimation**: Models light attenuation to estimate scene depth.\n",
        "- **LAB Stretching**: Enhances luminance and color balance in the LAB color space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import datetime\n",
        "import numpy as np\n",
        "import natsort\n",
        "import math\n",
        "from scipy import stats\n",
        "from skimage.color import rgb2hsv, hsv2rgb, rgb2lab, lab2rgb\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from functools import partial\n",
        "import ffmpeg\n",
        "\n",
        "# Configuration settings\n",
        "CONFIG = {\n",
        "    'input_dir': 'input',\n",
        "    'output_dir': 'output-ip',\n",
        "    'block_size': 9,\n",
        "    'gimfilt_radius': 30,\n",
        "    'eps': 1e-2,\n",
        "    'rb_compensation_flag': 0,  # 0: Compensate both Red and Blue, 1: Compensate only Red\n",
        "    'enhancement_strength': 0.6,  # Control enhancement intensity\n",
        "    'video_extensions': ['.mp4', '.avi', '.mov'],  # Supported video formats\n",
        "    'output_video_fps': 30,  # Default output video frame rate\n",
        "    'output_video_codec': 'mp4v',  # Codec for output video\n",
        "    'temp_video_path': 'temp_output.mp4',  # Temporary video file without audio\n",
        "}\n",
        "\n",
        "# Set numpy to ignore overflow warnings\n",
        "np.seterr(over='ignore')\n",
        "\n",
        "# Guided Filter Class\n",
        "class GuidedFilter:\n",
        "    \"\"\"Guided filter for image processing to refine transmission maps while preserving edges.\"\"\"\n",
        "    def __init__(self, input_image, radius=5, epsilon=0.4):\n",
        "        self._radius = 2 * radius + 1\n",
        "        self._epsilon = epsilon\n",
        "        self._input_image = self._to_float_img(input_image)\n",
        "        self._init_filter()\n",
        "\n",
        "    def _to_float_img(self, img):\n",
        "        if img.dtype == np.float32:\n",
        "            return img\n",
        "        return img.astype(np.float32) / 255.0\n",
        "\n",
        "    def _init_filter(self):\n",
        "        img = self._input_image\n",
        "        r = self._radius\n",
        "        eps = self._epsilon\n",
        "        ir, ig, ib = img[:, :, 0], img[:, :, 1], img[:, :, 2]\n",
        "        ksize = (r, r)\n",
        "        self._ir_mean = cv2.blur(ir, ksize)\n",
        "        self._ig_mean = cv2.blur(ig, ksize)\n",
        "        self._ib_mean = cv2.blur(ib, ksize)\n",
        "        irr = cv2.blur(ir * ir, ksize) - self._ir_mean ** 2 + eps\n",
        "        irg = cv2.blur(ir * ig, ksize) - self._ir_mean * self._ig_mean\n",
        "        irb = cv2.blur(ir * ib, ksize) - self._ir_mean * self._ib_mean\n",
        "        igg = cv2.blur(ig * ig, ksize) - self._ig_mean ** 2 + eps\n",
        "        igb = cv2.blur(ig * ib, ksize) - self._ig_mean * self._ib_mean\n",
        "        ibb = cv2.blur(ib * ib, ksize) - self._ib_mean ** 2 + eps\n",
        "        det = irr * (igg * ibb - igb * igb) - irg * (irg * ibb - igb * irb) + irb * (irg * igb - igg * irb)\n",
        "        self._irr_inv = (igg * ibb - igb * igb) / det\n",
        "        self._irg_inv = -(irg * ibb - igb * irb) / det\n",
        "        self._irb_inv = (irg * igb - igg * irb) / det\n",
        "        self._igg_inv = (irr * ibb - irb * irb) / det\n",
        "        self._igb_inv = -(irr * igb - irb * irg) / det\n",
        "        self._ibb_inv = (irr * igg - irg * irg) / det\n",
        "\n",
        "    def _compute_coefficients(self, input_p):\n",
        "        r = self._radius\n",
        "        ksize = (r, r)\n",
        "        ir, ig, ib = self._input_image[:, :, 0], self._input_image[:, :, 1], self._input_image[:, :, 2]\n",
        "        p_mean = cv2.blur(input_p, ksize)\n",
        "        ipr_cov = cv2.blur(ir * input_p, ksize) - self._ir_mean * p_mean\n",
        "        ipg_cov = cv2.blur(ig * input_p, ksize) - self._ig_mean * p_mean\n",
        "        ipb_cov = cv2.blur(ib * input_p, ksize) - self._ib_mean * p_mean\n",
        "        ar = self._irr_inv * ipr_cov + self._irg_inv * ipg_cov + self._irb_inv * ipb_cov\n",
        "        ag = self._irg_inv * ipr_cov + self._igg_inv * ipg_cov + self._igb_inv * ipb_cov\n",
        "        ab = self._irb_inv * ipr_cov + self._igb_inv * ipg_cov + self._ibb_inv * ipb_cov\n",
        "        b = p_mean - ar * self._ir_mean - ag * self._ig_mean - ab * self._ib_mean\n",
        "        return cv2.blur(ar, ksize), cv2.blur(ag, ksize), cv2.blur(ab, ksize), cv2.blur(b, ksize)\n",
        "\n",
        "    def _compute_output(self, ab):\n",
        "        ar_mean, ag_mean, ab_mean, b_mean = ab\n",
        "        ir, ig, ib = self._input_image[:, :, 0], self._input_image[:, :, 1], self._input_image[:, :, 2]\n",
        "        return ar_mean * ir + ag_mean * ig + ab_mean * ib + b_mean\n",
        "\n",
        "    def filter(self, input_p):\n",
        "        p_32f = self._to_float_img(input_p)\n",
        "        ab = self._compute_coefficients(p_32f)\n",
        "        return self._compute_output(ab)\n",
        "\n",
        "# Colour Correction Class\n",
        "class ColourCorrection:\n",
        "    \"\"\"Handles underwater image color correction by compensating for color casts and estimating depth.\"\"\"\n",
        "    def __init__(self, block_size=CONFIG['block_size'], gimfilt_radius=CONFIG['gimfilt_radius'], eps=CONFIG['eps']):\n",
        "        self.block_size = block_size\n",
        "        self.gimfilt_radius = gimfilt_radius\n",
        "        self.eps = eps\n",
        "\n",
        "    def _compensate_rb(self, image, flag):\n",
        "        b, g, r = cv2.split(image.astype(np.float64))\n",
        "        min_r, max_r = np.min(r), np.max(r)\n",
        "        min_g, max_g = np.min(g), np.max(g)\n",
        "        min_b, max_b = np.min(b), np.max(b)\n",
        "        if max_r == min_r or max_g == min_g or max_b == min_b:\n",
        "            return image\n",
        "        r = (r - min_r) / (max_r - min_r)\n",
        "        g = (g - min_g) / (max_g - min_g)\n",
        "        b = (b - min_b) / (max_b - min_b)\n",
        "        mean_r, mean_g, mean_b = np.mean(r), np.mean(g), np.mean(b)\n",
        "        compensation_strength = 0.4\n",
        "        if flag == 0:\n",
        "            r = (r + compensation_strength * (mean_g - mean_r) * (1 - r) * g) * max_r\n",
        "            b = (b + compensation_strength * (mean_g - mean_b) * (1 - b) * g) * max_b\n",
        "            g = g * max_g\n",
        "        elif flag == 1:\n",
        "            r = (r + compensation_strength * (mean_g - mean_r) * (1 - r) * g) * max_r\n",
        "            g = g * max_g\n",
        "            b = b * max_b\n",
        "        return cv2.merge([np.clip(b, 0, 255).astype(np.uint8),\n",
        "                         np.clip(g, 0, 255).astype(np.uint8),\n",
        "                         np.clip(r, 0, 255).astype(np.uint8)])\n",
        "\n",
        "    def _estimate_background_light(self, img, depth_map):\n",
        "        img = img.astype(np.float32) / 255.0 if img.dtype == np.uint8 else img\n",
        "        height, width = img.shape[:2]\n",
        "        n_bright = int(np.ceil(0.001 * height * width))\n",
        "        indices = np.argpartition(depth_map.ravel(), -n_bright)[-n_bright:]\n",
        "        candidates = img.reshape(-1, 3)[indices]\n",
        "        magnitudes = np.linalg.norm(candidates, axis=1)\n",
        "        sorted_indices = np.argsort(magnitudes)[::-1]\n",
        "        top_n = 10\n",
        "        top_candidates = candidates[sorted_indices[:top_n]]\n",
        "        atmospheric_light = np.mean(top_candidates, axis=0) * 255.0\n",
        "        return atmospheric_light\n",
        "\n",
        "    def _compute_depth_map(self, img):\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        x_1 = np.maximum(img[:, :, 0], img[:, :, 1])\n",
        "        x_2 = img[:, :, 2]\n",
        "        return 0.51157954 + 0.50516165 * x_1 - 0.90511117 * x_2\n",
        "\n",
        "    def _compute_min_depth(self, img, background_light):\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        background_light = background_light / 255.0\n",
        "        max_values = np.max(np.abs(img - background_light), axis=(0, 1)) / np.maximum(background_light, 1 - background_light)\n",
        "        return 1 - np.max(max_values)\n",
        "\n",
        "    def _global_stretching_depth(self, img_l):\n",
        "        flat = img_l.ravel()\n",
        "        indices = np.argsort(flat)\n",
        "        i_min, i_max = flat[indices[len(flat)//1000]], flat[indices[-len(flat)//1000]]\n",
        "        result = np.clip((img_l - i_min) / (i_max - i_min + 1e-10), 0, 1)\n",
        "        return cv2.GaussianBlur(result, (3, 3), 0.5)\n",
        "\n",
        "    def _get_rgb_transmission(self, depth_map):\n",
        "        return 0.98 ** depth_map, 0.97 ** depth_map, 0.88 ** depth_map\n",
        "\n",
        "    def _refine_transmission_map(self, transmission_b, transmission_g, transmission_r, img):\n",
        "        guided_filter = GuidedFilter(img, self.gimfilt_radius, self.eps)\n",
        "        transmission = np.stack([\n",
        "            guided_filter.filter(transmission_b),\n",
        "            guided_filter.filter(transmission_g),\n",
        "            guided_filter.filter(transmission_r)\n",
        "        ], axis=-1)\n",
        "        return transmission\n",
        "\n",
        "    def _compute_scene_radiance(self, img, transmission, atmospheric_light):\n",
        "        img = img.astype(np.float32)\n",
        "        min_transmission = 0.2\n",
        "        transmission = np.maximum(transmission, min_transmission)\n",
        "        scene_radiance = (img - atmospheric_light) / transmission + atmospheric_light\n",
        "        return np.clip(scene_radiance, 0, 255).astype(np.uint8)\n",
        "\n",
        "    def process(self, img, rb_compensation_flag=CONFIG['rb_compensation_flag']):\n",
        "        img_compensated = self._compensate_rb(img, rb_compensation_flag)\n",
        "        depth_map = self._compute_depth_map(img_compensated)\n",
        "        depth_map = self._global_stretching_depth(depth_map)\n",
        "        guided_filter = GuidedFilter(img_compensated, self.gimfilt_radius, self.eps)\n",
        "        refined_depth_map = guided_filter.filter(depth_map)\n",
        "        refined_depth_map = np.clip(refined_depth_map, 0, 1)\n",
        "        atmospheric_light = self._estimate_background_light(img_compensated, depth_map)\n",
        "        d_0 = self._compute_min_depth(img_compensated, atmospheric_light)\n",
        "        d_f = 6 * (depth_map + d_0)\n",
        "        transmission_b, transmission_g, transmission_r = self._get_rgb_transmission(d_f)\n",
        "        transmission = self._refine_transmission_map(transmission_b, transmission_g, transmission_r, img_compensated)\n",
        "        return self._compute_scene_radiance(img_compensated, transmission, atmospheric_light)\n",
        "\n",
        "# Image Enhancement Functions\n",
        "def cal_equalisation(img, ratio):\n",
        "    return np.clip(img * ratio, 0, 255)\n",
        "\n",
        "def rgb_equalisation(img):\n",
        "    img = img.astype(np.float32)\n",
        "    current_mean = np.mean(img, axis=(0, 1))\n",
        "    target_mean = 140\n",
        "    ratio = target_mean / (current_mean + 1e-10)\n",
        "    ratio = np.clip(ratio, 0.8, 1.2)\n",
        "    return cal_equalisation(img, ratio)\n",
        "\n",
        "def stretch_range(r_array, height, width):\n",
        "    flat = r_array.ravel()\n",
        "    mode = stats.mode(flat, keepdims=True).mode[0] if flat.size > 0 else np.median(flat)\n",
        "    mode_indices = np.where(flat == mode)[0]\n",
        "    mode_index_before = mode_indices[0] if mode_indices.size > 0 else len(flat) // 2\n",
        "    dr_min = (1 - 0.755) * mode\n",
        "    max_index = min(len(flat) - 1, len(flat) - int((len(flat) - mode_index_before) * 0.01))\n",
        "    sr_max = np.sort(flat)[max_index]\n",
        "    return dr_min, sr_max, mode\n",
        "\n",
        "def global_stretching_ab(a, height, width):\n",
        "    return a * (1.05 ** (1 - np.abs(a / 128)))\n",
        "\n",
        "def basic_stretching(img):\n",
        "    img = img.astype(np.float64)\n",
        "    min_vals = np.percentile(img, 2, axis=(0,1))\n",
        "    max_vals = np.percentile(img, 98, axis=(0,1))\n",
        "    range_vals = max_vals - min_vals\n",
        "    min_range = 50\n",
        "    mask = range_vals < min_range\n",
        "    max_vals[mask] = min_vals[mask] + min_range\n",
        "    img = np.clip((img - min_vals) * 255 / (max_vals - min_vals + 1e-10), 0, 255)\n",
        "    return img.astype(np.uint8)\n",
        "\n",
        "def global_stretching_luminance(img_l, height, width):\n",
        "    flat = img_l.ravel()\n",
        "    indices = np.argsort(flat)\n",
        "    i_min, i_max = flat[indices[len(flat)//50]], flat[indices[-len(flat)//50]]\n",
        "    if i_max == i_min:\n",
        "        i_min, i_max = flat.min(), flat.max()\n",
        "        if i_max == i_min:\n",
        "            return img_l\n",
        "    return np.clip((img_l - i_min) * 95 / (i_max - i_min + 1e-10), 0, 100)\n",
        "\n",
        "def lab_stretching(scene_radiance):\n",
        "    scene_radiance = np.clip(scene_radiance, 0, 255).astype(np.uint8)\n",
        "    original = scene_radiance.copy()\n",
        "    img_lab = rgb2lab(scene_radiance)\n",
        "    l, a, b = img_lab[:, :, 0], img_lab[:, :, 1], img_lab[:, :, 2]\n",
        "    img_lab[:, :, 0] = global_stretching_luminance(l, *scene_radiance.shape[:2])\n",
        "    img_lab[:, :, 1] = global_stretching_ab(a, *scene_radiance.shape[:2])\n",
        "    img_lab[:, :, 2] = global_stretching_ab(b, *scene_radiance.shape[:2])\n",
        "    enhanced = lab2rgb(img_lab) * 255\n",
        "    blend_factor = CONFIG['enhancement_strength']\n",
        "    result = blend_factor * enhanced + (1 - blend_factor) * original\n",
        "    return result\n",
        "\n",
        "def global_stretching_advanced(r_array, height, width, lambda_val, k_val):\n",
        "    flat = r_array.ravel()\n",
        "    indices = np.argsort(flat)\n",
        "    i_min, i_max = flat[indices[len(flat)//100]], flat[indices[-len(flat)//100]]\n",
        "    dr_min, sr_max, mode = stretch_range(r_array, height, width)\n",
        "    t_n = lambda_val ** 2\n",
        "    o_max_left = sr_max * t_n * k_val / mode\n",
        "    o_max_right = 255 * t_n * k_val / mode\n",
        "    dif = o_max_right - o_max_left\n",
        "    if dif >= 1:\n",
        "        indices = np.arange(1, int(dif) + 1)\n",
        "        sum_val = np.sum((1.326 + indices) * mode / (t_n * k_val))\n",
        "        dr_max = sum_val / int(dif)\n",
        "        p_out = np.where(r_array < i_min, (r_array - i_min) * (dr_min / i_min) + i_min,\n",
        "                         np.where(r_array > i_max, (r_array - dr_max) * (dr_max / i_max) + i_max,\n",
        "                                  ((r_array - i_min) * (255 - i_min) / (i_max - i_min) + i_min)))\n",
        "    else:\n",
        "        p_out = np.where(r_array < i_min, (r_array - r_array.min()) * (dr_min / r_array.min()) + r_array.min(),\n",
        "                         ((r_array - i_min) * (255 - dr_min) / (i_max - i_min) + dr_min))\n",
        "    return p_out\n",
        "\n",
        "def relative_stretching(scene_radiance, height, width):\n",
        "    scene_radiance = scene_radiance.astype(np.float64)\n",
        "    scene_radiance[:, :, 0] = global_stretching_advanced(scene_radiance[:, :, 0], height, width, 0.98, 1.1)\n",
        "    scene_radiance[:, :, 1] = global_stretching_advanced(scene_radiance[:, :, 1], height, width, 0.97, 1.1)\n",
        "    scene_radiance[:, :, 2] = global_stretching_advanced(scene_radiance[:, :, 2], height, width, 0.88, 0.9)\n",
        "    return scene_radiance\n",
        "\n",
        "def image_enhancement(scene_radiance):\n",
        "    if scene_radiance.shape[2] == 3:\n",
        "        scene_radiance = cv2.cvtColor(scene_radiance, cv2.COLOR_BGR2RGB)\n",
        "    if np.max(scene_radiance) == np.min(scene_radiance):\n",
        "        return scene_radiance\n",
        "    original = scene_radiance.copy()\n",
        "    scene_radiance = scene_radiance.astype(np.float64)\n",
        "    scene_radiance = basic_stretching(scene_radiance)\n",
        "    scene_radiance = lab_stretching(scene_radiance)\n",
        "    final_blend = 0.8\n",
        "    result = final_blend * scene_radiance + (1 - final_blend) * original\n",
        "    return np.clip(result, 0, 255).astype(np.uint8)\n",
        "\n",
        "# Processing Functions\n",
        "def process_image(file, input_dir=CONFIG['input_dir'], output_dir=CONFIG['output_dir']):\n",
        "    file_path = os.path.join(input_dir, file)\n",
        "    base_name, extension = os.path.splitext(file)\n",
        "    print(f'Processing image: {file}')\n",
        "    img = cv2.imread(file_path)\n",
        "    if img is None:\n",
        "        print(f\"Could not read image: {file}\")\n",
        "        return\n",
        "    colour_corrector = ColourCorrection()\n",
        "    print(\"Applying color correction...\")\n",
        "    corrected_img = colour_corrector.process(img)\n",
        "    print(\"Applying image enhancement...\")\n",
        "    final_result = image_enhancement(corrected_img)\n",
        "    final_result_bgr = cv2.cvtColor(final_result, cv2.COLOR_RGB2BGR)\n",
        "    output_file = os.path.join(output_dir, f'{base_name}_ColourCorrected{extension}')\n",
        "    cv2.imwrite(output_file, final_result_bgr)\n",
        "    print(f\"Completed processing image: {file}\")\n",
        "\n",
        "def process_video_frame(frame, colour_corrector):\n",
        "    corrected_frame = colour_corrector.process(frame)\n",
        "    enhanced_frame = image_enhancement(corrected_frame)\n",
        "    return cv2.cvtColor(enhanced_frame, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "def process_video(file, input_dir=CONFIG['input_dir'], output_dir=CONFIG['output_dir']):\n",
        "    file_path = os.path.join(input_dir, file)\n",
        "    prefix = file.split('.')[0]\n",
        "    print(f'Processing video: {file}')\n",
        "    cap = cv2.VideoCapture(file_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Could not open video: {file}\")\n",
        "        return\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or CONFIG['output_video_fps']\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    temp_output_path = os.path.join(output_dir, CONFIG['temp_video_path'])\n",
        "    final_output_path = os.path.join(output_dir, f'{prefix}_ColourCorrected.mp4')\n",
        "    fourcc = cv2.VideoWriter_fourcc(*CONFIG['output_video_codec'])\n",
        "    out = cv2.VideoWriter(temp_output_path, fourcc, fps, (width, height))\n",
        "    if not out.isOpened():\n",
        "        print(f\"Could not create output video: {temp_output_path}\")\n",
        "        cap.release()\n",
        "        return\n",
        "    colour_corrector = ColourCorrection()\n",
        "    print(f\"Processing {frame_count} frames...\")\n",
        "    frame_idx = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        print(f\"Processing frame {frame_idx + 1}/{frame_count}\")\n",
        "        processed_frame = process_video_frame(frame, colour_corrector)\n",
        "        out.write(processed_frame)\n",
        "        frame_idx += 1\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    try:\n",
        "        print(f\"Merging original audio into: {final_output_path}\")\n",
        "        input_video = ffmpeg.input(temp_output_path)\n",
        "        input_audio = ffmpeg.input(file_path).audio\n",
        "        output = ffmpeg.output(input_video.video, input_audio, final_output_path, vcodec='copy', acodec='copy', strict='experimental')\n",
        "        ffmpeg.run(output, overwrite_output=True)\n",
        "        print(f\"Completed processing video with audio: {file}\")\n",
        "        if os.path.exists(temp_output_path):\n",
        "            os.remove(temp_output_path)\n",
        "    except ffmpeg.Error as e:\n",
        "        print(f\"Error merging audio: {e.stderr.decode()}\")\n",
        "        return\n",
        "\n",
        "def main_color_correction():\n",
        "    \"\"\"Main function to process images and videos in the input directory for color correction.\"\"\"\n",
        "    os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
        "    if not os.access(CONFIG['output_dir'], os.W_OK):\n",
        "        print(f\"No write permissions for directory {CONFIG['output_dir']}\")\n",
        "        exit(1)\n",
        "    start_time = datetime.datetime.now()\n",
        "    files = natsort.natsorted(os.listdir(CONFIG['input_dir']))\n",
        "    files = [f for f in files if os.path.isfile(os.path.join(CONFIG['input_dir'], f))]\n",
        "    image_files = [f for f in files if os.path.splitext(f)[1].lower() not in CONFIG['video_extensions']]\n",
        "    video_files = [f for f in files if os.path.splitext(f)[1].lower() in CONFIG['video_extensions']]\n",
        "    if image_files:\n",
        "        print(f\"Processing {len(image_files)} images...\")\n",
        "        with Pool(processes=cpu_count()) as pool:\n",
        "            pool.map(process_image, image_files)\n",
        "    if video_files:\n",
        "        print(f\"Processing {len(video_files)} videos...\")\n",
        "        for video_file in video_files:\n",
        "            process_video(video_file)\n",
        "    print(f'Total processing time: {datetime.datetime.now() - start_time}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Run Color Correction\n",
        "\n",
        "Execute the color correction pipeline to process images and videos in the `input/` directory. The processed files are saved in the `output-ip/` directory with a `_ColourCorrected` suffix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "main_color_correction()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Detail Enhancement with Real-ESRGAN\n",
        "\n",
        "This section applies the **Real-ESRGAN** model to enhance the details of color-corrected images. Real-ESRGAN uses a deep learning-based super-resolution technique to improve image sharpness and clarity.\n",
        "\n",
        "**Note**: Ensure the pre-trained model `net_g_5000.pth` is uploaded to `/content/` before running this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
        "from realesrgan import RealESRGANer\n",
        "\n",
        "def main_esrgan():\n",
        "    # Configuration\n",
        "    input_path = '/content/output-ip'  # Input folder with color-corrected images\n",
        "    output_path = '/content/output'  # Output folder for enhanced images\n",
        "    model_name = 'RealESRGAN_x4plus'\n",
        "    model_path = '/content/net_g_5000.pth'  # Path to pre-trained model\n",
        "    outscale = 1  # Upsampling scale (1 for no upscaling, just enhancement)\n",
        "    suffix = 'out'  # Suffix for enhanced images\n",
        "    tile = 1000  # Tile size for processing large images\n",
        "\n",
        "    # Validate model name\n",
        "    if model_name != 'RealESRGAN_x4plus':\n",
        "        raise ValueError('This script only supports RealESRGAN_x4plus model')\n",
        "\n",
        "    # Initialize Real-ESRGAN model\n",
        "    model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n",
        "    netscale = 4\n",
        "    upsampler = RealESRGANer(\n",
        "        scale=netscale,\n",
        "        model_path=model_path,\n",
        "        model=model,\n",
        "        tile=tile,\n",
        "        tile_pad=10,\n",
        "        pre_pad=0,\n",
        "        half=False  # Use fp32 precision\n",
        "    )\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    # Get input files\n",
        "    if os.path.isfile(input_path):\n",
        "        paths = [input_path]\n",
        "    else:\n",
        "        paths = sorted(glob.glob(os.path.join(input_path, '*')))\n",
        "\n",
        "    # Start total execution timer\n",
        "    total_start_time = time.time()\n",
        "\n",
        "    # Process each image\n",
        "    for idx, path in enumerate(paths):\n",
        "        imgname, extension = os.path.splitext(os.path.basename(path))\n",
        "        print(f'Processing {idx}: {imgname}')\n",
        "\n",
        "        start_time = time.time()\n",
        "        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
        "        if img is None:\n",
        "            print(f'Failed to load image: {path}')\n",
        "            continue\n",
        "\n",
        "        img_mode = 'RGBA' if len(img.shape) == 3 and img.shape[2] == 4 else None\n",
        "        try:\n",
        "            output, _ = upsampler.enhance(img, outscale=outscale)\n",
        "        except RuntimeError as error:\n",
        "            print(f'Error processing {imgname}: {error}')\n",
        "            print('Try reducing tile size if you encounter CUDA out of memory.')\n",
        "            continue\n",
        "\n",
        "        extension = extension[1:] if img_mode != 'RGBA' else 'png'\n",
        "        save_path = os.path.join(output_path, f'{imgname}_{suffix}.{extension}')\n",
        "        cv2.imwrite(save_path, output)\n",
        "\n",
        "        end_time = time.time()\n",
        "        processing_time = end_time - start_time\n",
        "        print(f'Saved: {save_path}')\n",
        "        print(f'Processing time for {imgname}: {processing_time:.2f} seconds')\n",
        "\n",
        "    total_end_time = time.time()\n",
        "    total_time = total_end_time - total_start_time\n",
        "    print(f'Total execution time: {total_time:.2f} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Run Real-ESRGAN Enhancement\n",
        "\n",
        "Execute the Real-ESRGAN pipeline to enhance the details of color-corrected images in the `output-ip/` directory. The enhanced images are saved in the `output/` directory with an `_out` suffix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "main_esrgan()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Evaluate Image Quality\n",
        "\n",
        "This section evaluates the quality of the processed images by comparing them to reference images in the `reference/` directory. It calculates two metrics:\n",
        "- **PSNR (Peak Signal-to-Noise Ratio)**: Measures the quality of reconstruction (higher is better).\n",
        "- **MSE (Mean Squared Error)**: Measures the average squared difference between images (lower is better).\n",
        "\n",
        "Results are saved to `metrics_output_ip.txt` (for color-corrected images) and `metrics_output.txt` (for Real-ESRGAN enhanced images)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "from skimage.metrics import peak_signal_noise_ratio, mean_squared_error\n",
        "\n",
        "def compare_images(ref_folder, target_folder, suffix, output_file):\n",
        "    with open(output_file, 'w') as f:\n",
        "        f.write(f\"{'File':<30} {'PSNR (dB)':<12} {'MSE':<10}\\n\")\n",
        "        f.write('-' * 60 + '\\n')\n",
        "        ref_files = [f for f in os.listdir(ref_folder) if os.path.isfile(os.path.join(ref_folder, f))]\n",
        "        for ref_file in ref_files:\n",
        "            base_name, ext = os.path.splitext(ref_file)\n",
        "            target_file = f\"{base_name}{suffix}{ext}\"\n",
        "            target_path = os.path.join(target_folder, target_file)\n",
        "            if os.path.exists(target_path):\n",
        "                ref_img = cv2.imread(os.path.join(ref_folder, ref_file))\n",
        "                target_img = cv2.imread(target_path)\n",
        "                if ref_img is None or target_img is None:\n",
        "                    f.write(f\"{ref_file:<30} {'Error loading images':<12}\\n\")\n",
        "                    continue\n",
        "                try:\n",
        "                    psnr = peak_signal_noise_ratio(ref_img, target_img)\n",
        "                    mse = mean_squared_error(ref_img, target_img)\n",
        "                    f.write(f\"{ref_file:<30} {psnr:<12.2f} {mse:<10.2f}\\n\")\n",
        "                except Exception as e:\n",
        "                    f.write(f\"{ref_file:<30} {'Error':<12} {str(e):<10}\\n\")\n",
        "            else:\n",
        "                f.write(f\"{ref_file:<30} {'Target not found':<12}\\n\")\n",
        "\n",
        "# Define folder paths and suffixes\n",
        "reference_folder = 'reference/'\n",
        "output_ip_folder = 'output-ip/'\n",
        "output_folder = 'output/'\n",
        "suffix_ip = '_ColourCorrected'\n",
        "suffix_out = '_ColourCorrected_out'\n",
        "\n",
        "# Compare images\n",
        "compare_images(reference_folder, output_ip_folder, suffix_ip, 'metrics_output_ip.txt')\n",
        "compare_images(reference_folder, output_folder, suffix_out, 'metrics_output.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: View Evaluation Results\n",
        "\n",
        "Display the contents of the evaluation metrics files to review the PSNR and MSE values for both color-corrected and Real-ESRGAN-enhanced images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!cat metrics_output_ip.txt\n",
        "!echo \"\\n\"\n",
        "!cat metrics_output.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrates a hybrid approach to underwater image enhancement by combining traditional color correction techniques with Real-ESRGAN for detail enhancement. The pipeline effectively addresses underwater imaging challenges such as color distortion and low contrast, producing visually appealing results. The evaluation metrics provide quantitative insights into the improvement achieved at each stage.\n",
        "\n",
        "**Next Steps**:\n",
        "- Experiment with different `CONFIG` parameters (e.g., `enhancement_strength`, `gimfilt_radius`) to optimize results for specific underwater conditions.\n",
        "- Adjust the `outscale` parameter in Real-ESRGAN for upscaling if higher resolution is desired."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
